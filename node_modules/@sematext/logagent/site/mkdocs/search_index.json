{
    "docs": [
        {
            "location": "/", 
            "text": "- \nread more\n\n\n@sematext/logagent\n\n\nSmart and lightweight Log Parser and Log Shipper written in Node. It can ship logs to Elasticsearch and thus also to \nLogsene\n. See \nDocumentation\n.\n\n\nNote: Previous documentation \nv1.x\n\n\nFeatures\n\n\nThis project contains a library and patterns for log parsing and cli tools and installers to use logagent-js as log shipper with the following features: \n\n\nParser\n\n\n\n\nlog format detection and intelligent pattern matching \n\n\npattern library included \n\n\neasy to extend with custom patterns and JS transform functions\n\n\nrecognition of Date and Number fields\n\n\nreplace sensitive data with SHA-1 hash codes\n\n\nGeoIP lookup with automatic GeoIP db updates (maxmind geopip-lite files)\n\n\n\n\nCommand Line Tool\n\n\n\n\nlog format converter (e.g. text to JSON, line delimited JSON or YAML)\n\n\n\n\nlog shipper for \nLogsene\n and Elasticsearch\n\n\n\n\n\n\nservice installer for launchd (Mac OS X), upstart and systemd (Linux) \n\n\n\n\ndisk buffer for failed inserts during network outage\n\n\n\n\nInputs\n\n\n\n\nStandard input (stdin) that can read the output stream from any Linux cli tool\n\n\npatterns are applied to each incoming text line; includes support for multi-line patters, e.g. for Java Stack Traces and JSON input.\n\n\nSyslog Server (UDP) listener - logagent-js can also act as a syslog server and receive Syslog messages via UDP. The parser is applied to the message field. \n\n\nHeroku Log Drain\n makes it easy to ship Heroku logs to Elasticsearch or \nLogsene\n\n\nCloud Foundry Log Drain\n\n\n\n\nProcessing\n\n\n\n\nlogagent-js applies patterns defined in patterns.yml to all logs and creates structured logs from plain-text log lines\n\n\nGeoIP lookups for IP address fields, including automatic download and update of the GeoIP lite database from Maxmind\n\n\n\n\nReliable log shipping with disk buffer\n\n\nLogagent doesn't lose data.  It stores parsed logs to a disk buffer if the network connection to the Elasticsearch API fails.  Logagent retries shipping logs later, when the network or Elasticsearch is available again.  \n\n\nOutputs\n\n\n\n\nbulk inserts to Elasticsearch and \nLogsene\n / Elasticsearch API\n\n\nJSON, line delimited JSON and YAML to standard output \n\n\nUDP forwarding to rtail server for realtime log view \n\n\n\n\nDeployment options\n\n\n\n\nDeployable as a system service: systemd, upstart (Linux), or launchd (Mac OS X)\n\n\nDocker Container to receive logs via syslog\n\n\nDeployement to Heroku as Heroku Log drain\n\n\nDeployement to Cloud Foundry as Cloud Foundry Log drain (thus usable with Pivotal, Bluemix, etc.)\n\n\n\n\nAPI\n\n\n\n\nNode.js module to integrate parsers into Node.js programs\n\n\nlogagent-js is a part of \nSPM for Docker\n to parse Container Logs\n\n\n\n\nRelated packages\n\n\n\n\nSematext Agent for Docker\n - collects metrics, events and logs from Docker API and CoreOS. Logagent-js is a component of sematext-agent-docker. More Information: \nInnovative Docker Log Management\n\n\nLogsene-CLI\n - Enables searching Logsene log entries from the command-line. \n\n\nSPM Agent for Node.js\n - collects performance metrics for Node and io.js applications\n\n\nCustom Metrics\n - Custom Metrics for SPM \n\n\nWinston-Logsene\n - Logging for Node.js - Winston transport layer for Logsene\n\n\n\n\nSupport\n\n\n\n\nTwitter: \n@sematext\n\n\nBlog: \nsematext.com/blog\n\n\nHomepage: \nsematext.com", 
            "title": "Features and Overview"
        }, 
        {
            "location": "/#sematextlogagent", 
            "text": "Smart and lightweight Log Parser and Log Shipper written in Node. It can ship logs to Elasticsearch and thus also to  Logsene . See  Documentation .  Note: Previous documentation  v1.x", 
            "title": "@sematext/logagent"
        }, 
        {
            "location": "/#features", 
            "text": "This project contains a library and patterns for log parsing and cli tools and installers to use logagent-js as log shipper with the following features:", 
            "title": "Features"
        }, 
        {
            "location": "/#parser", 
            "text": "log format detection and intelligent pattern matching   pattern library included   easy to extend with custom patterns and JS transform functions  recognition of Date and Number fields  replace sensitive data with SHA-1 hash codes  GeoIP lookup with automatic GeoIP db updates (maxmind geopip-lite files)", 
            "title": "Parser"
        }, 
        {
            "location": "/#command-line-tool", 
            "text": "log format converter (e.g. text to JSON, line delimited JSON or YAML)   log shipper for  Logsene  and Elasticsearch    service installer for launchd (Mac OS X), upstart and systemd (Linux)    disk buffer for failed inserts during network outage", 
            "title": "Command Line Tool"
        }, 
        {
            "location": "/#inputs", 
            "text": "Standard input (stdin) that can read the output stream from any Linux cli tool  patterns are applied to each incoming text line; includes support for multi-line patters, e.g. for Java Stack Traces and JSON input.  Syslog Server (UDP) listener - logagent-js can also act as a syslog server and receive Syslog messages via UDP. The parser is applied to the message field.   Heroku Log Drain  makes it easy to ship Heroku logs to Elasticsearch or  Logsene  Cloud Foundry Log Drain", 
            "title": "Inputs"
        }, 
        {
            "location": "/#processing", 
            "text": "logagent-js applies patterns defined in patterns.yml to all logs and creates structured logs from plain-text log lines  GeoIP lookups for IP address fields, including automatic download and update of the GeoIP lite database from Maxmind", 
            "title": "Processing"
        }, 
        {
            "location": "/#reliable-log-shipping-with-disk-buffer", 
            "text": "Logagent doesn't lose data.  It stores parsed logs to a disk buffer if the network connection to the Elasticsearch API fails.  Logagent retries shipping logs later, when the network or Elasticsearch is available again.", 
            "title": "Reliable log shipping with disk buffer"
        }, 
        {
            "location": "/#outputs", 
            "text": "bulk inserts to Elasticsearch and  Logsene  / Elasticsearch API  JSON, line delimited JSON and YAML to standard output   UDP forwarding to rtail server for realtime log view", 
            "title": "Outputs"
        }, 
        {
            "location": "/#deployment-options", 
            "text": "Deployable as a system service: systemd, upstart (Linux), or launchd (Mac OS X)  Docker Container to receive logs via syslog  Deployement to Heroku as Heroku Log drain  Deployement to Cloud Foundry as Cloud Foundry Log drain (thus usable with Pivotal, Bluemix, etc.)", 
            "title": "Deployment options"
        }, 
        {
            "location": "/#api", 
            "text": "Node.js module to integrate parsers into Node.js programs  logagent-js is a part of  SPM for Docker  to parse Container Logs", 
            "title": "API"
        }, 
        {
            "location": "/#related-packages", 
            "text": "Sematext Agent for Docker  - collects metrics, events and logs from Docker API and CoreOS. Logagent-js is a component of sematext-agent-docker. More Information:  Innovative Docker Log Management  Logsene-CLI  - Enables searching Logsene log entries from the command-line.   SPM Agent for Node.js  - collects performance metrics for Node and io.js applications  Custom Metrics  - Custom Metrics for SPM   Winston-Logsene  - Logging for Node.js - Winston transport layer for Logsene", 
            "title": "Related packages"
        }, 
        {
            "location": "/#support", 
            "text": "Twitter:  @sematext  Blog:  sematext.com/blog  Homepage:  sematext.com", 
            "title": "Support"
        }, 
        {
            "location": "/installation/", 
            "text": "Install Node.js\n\n\nOfficial Node.js \ndownloads and instructions\n.\nE.g. for Debian/Ubuntu:\n\n\ncurl -sL https://deb.nodesource.com/setup_4.x | sudo -E bash -\nsudo apt-get install -y nodejs\n\n\n\n\nInstall Logagent\n\n\nsudo npm i -g @sematext/logagent \n\n\n\n\nInstall service (Linux, Mac OS X)\n\n\n\n\nGet a free account at \nsematext.com/spm\n\n\ncreate a Logsene App\n to obtain an App Token for \nLogsene\n \n\n\nInstall logagent as system service\nLogagent detects the init system and installs systemd or upstart service scripts. \nOn Mac OS X it creates a launchd service. Simply run:\n\n\n\n\n# install logagent package globally \nsudo npm i -g @sematext/logagent\nsudo logagent-setup LOGSENE_TOKEN\n\n\n\n\nThe setup script generates the configuraton file in \n/etc/sematext/logagent.conf\n.\nThe default settings ship all logs from \n/var/log/**/*.log\n to Logsene. \n\n\nLocation of service scripts:\n- upstart: /etc/init/logagent.conf\n- systemd: /etc/systemd/system/logagent.service\n- launchd: /Library/LaunchDaemons/com.sematext.logagent.plist\n\n\nStart/stop service: \n- upstart: \nservice logagent stop/start\n\n- systemd: \nsystemctl stop/start logagent\n\n- launchd: \nlaunchctl start/stop com.sematext.logagent\n\n\nLogagent in a Docker Container as Syslog Listener\n\n\nYou can build a Docker image with logagent running in it and activing as a Syslog UDP listener.  Then you can run this as a container.  Once you have this \"containerized logagent\" you can start all yoour other containers with Syslog driver and point it to the \"containerized logagent's\" UDP port (514).  Here are the steps:\n\n\nBuild the docker image, and then run logagent inside it with the given LOGSENE_TOKEN\n\n\ngit clone https://github.com/sematext/logagent-js.git\ncd logagent-js\ndocker build -t logagent . \ndocker run -p 514:514/udp -e LOGSENE_TOKEN=YOUR_LOGSENE_TOKEN -d --name logagent --restart=always logagent\n\n\n\n\nRun your other containers with Syslog driver\n\n\nexport $DOCKER_HOSTNAME=192.168.99.100\ndocker run --log-driver=syslog --log-opt syslog-address=udp://$DOCKER_HOSTNAME:514 --log-opt tag=\n{{.ImageName}}#{{.Name}}#{{.ID}}\n -p 9003:80 -d nginx\ncurl $DOCKER_HOSTNAME:9003\n\n\n\n\nContainer Options\n\n- Pass a custom pattern file\n\n\n-v $PWD/patterns.yml:/patterns.yml -e PATTERN_FILE=/patterns.yml\n\n\n\n\n\n\nSet any CLI option\ne.g. print logs in YML format to console (default is \"-s\" - silent)\n\n\n\n\n-e LOGAGENT_OPTIONS=\n-y\n\n\n\n\n\nTo view realtime logs in the Web Browser with rtail, simply add the options for rtail and open the http port for rtail-server UI. Please note: \nrtail UI might be slow for high log volumes\n\n\nexport LOGAGENT_OPTIONS=\n-s --rtail-host $HOSTNAME --rtail-web-port 80 --rtail-port 9999\n\ndocker run -p 8080:80 -p 514:514/udp -e LOGAGENT_OPTIONS -e LOGSENE_TOKEN=YOUR_LOGSENE_TOKEN -d --name logagent --restart=always logagent\n\n\n\n\n\n\nSet Node.js Memory limits\n\n\n\n\n-e NODE_OPTIONS=\n--max-old-space-size=200\n\n\n\n\n\nPlease note \nSematext Agent Docker\n might be of interest if you like to collect logs, events and metrics from Docker. \n\n\nRun Logagent as Heroku Log Drain\n\n\nYou can forward your \nHeroku\n logs to Logsene using Heroku \nLog Drain\n like this:\n\n\nheroku drain:add --app HerokuAppName URL\n\n\n\n\nHere are the steps:\n\n\nTo ship your Heroku logs to Logsene or Elasticsearch deploy Logagent on Heroku. It will act as an HTTPS log drain. \n\n\n\n\nGet a free account \napps.sematext.com\n\n\nCreate a \nLogsene\n App to obtain the Logsene Token\n\n\nDeploy logagent-js to Heroku using the Deploy to Heroku button\n\n\n\n\n \n... or use the following commands:\n\n\ngit clone https://github.com/sematext/logagent-js.git\n  cd logagent-js\n  heroku login \n  heroku create\n  git push heroku master\n\n4. Add the log drain using the URL format like https://loggerAppName.herokuapps.com/LOGSENE_TOKEN.\n  Use the following command to grab the dynamically assigned name from \"heroku create\" command.\n\n\nexport LOGSENE_TOKEN=YOUR_LOGSENE_TOKEN\n  heroku drains:add --app YOUR_HEROKU_MAIN_APPLICATION `heroku info -s | grep web-url | cut -d= -f2`$LOGSENE_TOKEN\n\nNow you can see your logs in Logsene, define Alert-Queries or use Kibana for Dashboards. \n\n\n\n\nScale logagent-js service on Heroku\n\n\n\n\nIn case of high log volume, scale logagent-js on demand using \n\n\nheroku scale web=3\n\n\n\n\nSee also:\n- \nHow to Ship Heroku Logs to Logsene / Managed ELK Stack", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#install-nodejs", 
            "text": "Official Node.js  downloads and instructions .\nE.g. for Debian/Ubuntu:  curl -sL https://deb.nodesource.com/setup_4.x | sudo -E bash -\nsudo apt-get install -y nodejs", 
            "title": "Install Node.js"
        }, 
        {
            "location": "/installation/#install-logagent", 
            "text": "sudo npm i -g @sematext/logagent", 
            "title": "Install Logagent"
        }, 
        {
            "location": "/installation/#install-service-linux-mac-os-x", 
            "text": "Get a free account at  sematext.com/spm  create a Logsene App  to obtain an App Token for  Logsene    Install logagent as system service\nLogagent detects the init system and installs systemd or upstart service scripts. \nOn Mac OS X it creates a launchd service. Simply run:   # install logagent package globally \nsudo npm i -g @sematext/logagent\nsudo logagent-setup LOGSENE_TOKEN  The setup script generates the configuraton file in  /etc/sematext/logagent.conf .\nThe default settings ship all logs from  /var/log/**/*.log  to Logsene.   Location of service scripts:\n- upstart: /etc/init/logagent.conf\n- systemd: /etc/systemd/system/logagent.service\n- launchd: /Library/LaunchDaemons/com.sematext.logagent.plist  Start/stop service: \n- upstart:  service logagent stop/start \n- systemd:  systemctl stop/start logagent \n- launchd:  launchctl start/stop com.sematext.logagent", 
            "title": "Install service (Linux, Mac OS X)"
        }, 
        {
            "location": "/installation/#logagent-in-a-docker-container-as-syslog-listener", 
            "text": "You can build a Docker image with logagent running in it and activing as a Syslog UDP listener.  Then you can run this as a container.  Once you have this \"containerized logagent\" you can start all yoour other containers with Syslog driver and point it to the \"containerized logagent's\" UDP port (514).  Here are the steps:  Build the docker image, and then run logagent inside it with the given LOGSENE_TOKEN  git clone https://github.com/sematext/logagent-js.git\ncd logagent-js\ndocker build -t logagent . \ndocker run -p 514:514/udp -e LOGSENE_TOKEN=YOUR_LOGSENE_TOKEN -d --name logagent --restart=always logagent  Run your other containers with Syslog driver  export $DOCKER_HOSTNAME=192.168.99.100\ndocker run --log-driver=syslog --log-opt syslog-address=udp://$DOCKER_HOSTNAME:514 --log-opt tag= {{.ImageName}}#{{.Name}}#{{.ID}}  -p 9003:80 -d nginx\ncurl $DOCKER_HOSTNAME:9003  Container Options \n- Pass a custom pattern file  -v $PWD/patterns.yml:/patterns.yml -e PATTERN_FILE=/patterns.yml   Set any CLI option\ne.g. print logs in YML format to console (default is \"-s\" - silent)   -e LOGAGENT_OPTIONS= -y   To view realtime logs in the Web Browser with rtail, simply add the options for rtail and open the http port for rtail-server UI. Please note:  rtail UI might be slow for high log volumes  export LOGAGENT_OPTIONS= -s --rtail-host $HOSTNAME --rtail-web-port 80 --rtail-port 9999 \ndocker run -p 8080:80 -p 514:514/udp -e LOGAGENT_OPTIONS -e LOGSENE_TOKEN=YOUR_LOGSENE_TOKEN -d --name logagent --restart=always logagent   Set Node.js Memory limits   -e NODE_OPTIONS= --max-old-space-size=200   Please note  Sematext Agent Docker  might be of interest if you like to collect logs, events and metrics from Docker.", 
            "title": "Logagent in a Docker Container as Syslog Listener"
        }, 
        {
            "location": "/installation/#run-logagent-as-heroku-log-drain", 
            "text": "You can forward your  Heroku  logs to Logsene using Heroku  Log Drain  like this:  heroku drain:add --app HerokuAppName URL  Here are the steps:  To ship your Heroku logs to Logsene or Elasticsearch deploy Logagent on Heroku. It will act as an HTTPS log drain.    Get a free account  apps.sematext.com  Create a  Logsene  App to obtain the Logsene Token  Deploy logagent-js to Heroku using the Deploy to Heroku button    \n... or use the following commands:  git clone https://github.com/sematext/logagent-js.git\n  cd logagent-js\n  heroku login \n  heroku create\n  git push heroku master \n4. Add the log drain using the URL format like https://loggerAppName.herokuapps.com/LOGSENE_TOKEN.\n  Use the following command to grab the dynamically assigned name from \"heroku create\" command.  export LOGSENE_TOKEN=YOUR_LOGSENE_TOKEN\n  heroku drains:add --app YOUR_HEROKU_MAIN_APPLICATION `heroku info -s | grep web-url | cut -d= -f2`$LOGSENE_TOKEN \nNow you can see your logs in Logsene, define Alert-Queries or use Kibana for Dashboards.    Scale logagent-js service on Heroku   In case of high log volume, scale logagent-js on demand using   heroku scale web=3  See also:\n-  How to Ship Heroku Logs to Logsene / Managed ELK Stack", 
            "title": "Run Logagent as Heroku Log Drain"
        }, 
        {
            "location": "/windows/", 
            "text": "Installation on windows\n\n\n1) Download and install node\n\n\nDownload nodejs from \nnodejs.org\n and execute the installer. \n\n\n2) Install Logagent and windows event plugin\n\n\nnpm i -g @sematext/logagent\nnpm i -g logagent-input-windows-events\n# run logagent windows version \nlogagent-windows --help \n\n\n\n\n3) Optional service installer\n\n\nCreate a configuration file for Logagent in \n\n\n%ProgramData%\\Sematext\\logagent.conf\n\n\n\n\n(default: c:\\ProgramData\\sematext\\logagent.conf)\n\n\nIn case you want to store the configuration file in a different directory, enter the new location in the registry:\n\n\nHKEY_LOCAL_MACHINE\\System\\CurrentControlSet\\Control\\Session Manager\\Environment\\LOGAGENT_CONFIG\n\n\n\n\nExample config file to collect Windows events to Elasticsearch: \n\n\noptions:\n  includeOriginalLine: false\n  suppress: true\n\ninput:\n  windowsEvent:\n    module: logagent-input-windows-events \n    # query events every 10 seconds\n    interval: 10\n    maxEvents: 1000\n\noutput:  \n  local-es:\n    module: elasticsearch\n    url: http://localhost:9200\n    index: windows_events\n\n\n\n\nRun service installer: \n\n\nlogagent-windows -install\n\n\n\n\nTo uninstall the service run \n\n\nlogagent-windows -uninstall", 
            "title": "Windows Installation"
        }, 
        {
            "location": "/windows/#installation-on-windows", 
            "text": "1) Download and install node  Download nodejs from  nodejs.org  and execute the installer.   2) Install Logagent and windows event plugin  npm i -g @sematext/logagent\nnpm i -g logagent-input-windows-events\n# run logagent windows version \nlogagent-windows --help   3) Optional service installer  Create a configuration file for Logagent in   %ProgramData%\\Sematext\\logagent.conf  (default: c:\\ProgramData\\sematext\\logagent.conf)  In case you want to store the configuration file in a different directory, enter the new location in the registry:  HKEY_LOCAL_MACHINE\\System\\CurrentControlSet\\Control\\Session Manager\\Environment\\LOGAGENT_CONFIG  Example config file to collect Windows events to Elasticsearch:   options:\n  includeOriginalLine: false\n  suppress: true\n\ninput:\n  windowsEvent:\n    module: logagent-input-windows-events \n    # query events every 10 seconds\n    interval: 10\n    maxEvents: 1000\n\noutput:  \n  local-es:\n    module: elasticsearch\n    url: http://localhost:9200\n    index: windows_events  Run service installer:   logagent-windows -install  To uninstall the service run   logagent-windows -uninstall", 
            "title": "Installation on windows"
        }, 
        {
            "location": "/cli-parameters/", 
            "text": "Command Line Parameters\n\n\nSynopsis\n\n\nlogagent [options] [file list]\n\n\n\n\n\n\n\n\nOptions\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGenernal options\n\n\n\n\n\n\n\n\n-h, --help\n\n\noutput logagent help\n\n\n\n\n\n\n-V, --version\n\n\noutput logagent version\n\n\n\n\n\n\n-v, --verbose\n\n\noutput activity report every minute\n\n\n\n\n\n\n--config \n\n\npath to logagent config file (see below)\n\n\n\n\n\n\n--geoipEnabled \n\n\ntrue/false to enable/disable geoip lookups in patterns.\n\n\n\n\n\n\n--diskBufferDir  path\n\n\ndirectory to store status and buffered logs (during network outage)\n\n\n\n\n\n\n--includeOriginalLine\n\n\nincludes the original message in parsed logs\n\n\n\n\n\n\n-f, --file \n\n\nfile with pattern definitions, use multiple -f options multiple files\n\n\n\n\n\n\n-s, --suppress\n\n\nsilent, print no logs to stdout, prints only stats on exit\n\n\n\n\n\n\n--printStats\n\n\nprint processing stats in the given interval in seconds, e.g. \n--print_stats 30\n to stderr. Usefull with -s to see logagent activity on the console without printing the parsed logs to stdout.\n\n\n\n\n\n\nLog input options\n\n\n\n\n\n\n\n\n-g glob-pattern\n\n\nuse a \nglob\n pattern to watch log files e.g. \n-g \"{/var/log/*.log,/Users/stefan/myapp/*.log}\"\n. The complete glob expression must be quoted, to avoid interpretation of special characters by the linux shell.\n\n\n\n\n\n\n+\n\n\n--tailStartPosition bytes\n\n\n\n\n\n\n--stdin\n\n\nread from stdin, default if no other input like files or UDP are set\n\n\n\n\n\n\n-n name\n\n\nname for the log source only when stdin is used, important to make multi-line patterns working on stdin because the status is tracked by the log source name.\n\n\n\n\n\n\n-u UDP_PORT\n\n\nstarts a syslogd UDP listener on the given port to act as syslogd\n\n\n\n\n\n\n--heroku PORT\n\n\nlistens for Heroku logs (http drain / framed syslog over http)\n\n\n\n\n\n\n--cfhttp PORT\n\n\nlistens for Cloud Foundry logs (syslog over http)\n\n\n\n\n\n\nlist of files\n\n\nEvery argument after the options list is interpreted as file name. All files in the file list (e.g. /var/log/*.log) are watched by \ntail-forever\n starting at end of file\n\n\n\n\n\n\nOutput options\n\n\n\n\n\n\n\n\nstandard output stream\n\n\ncombine logagent with any unix tool via pipes\n\n\n\n\n\n\n-y, --yaml\n\n\nprints parsed messages in YAML format to stdout\n\n\n\n\n\n\n-p, --pretty\n\n\nprints parsed messages in pretty json format to stdout\n\n\n\n\n\n\n-j, --ldjson\n\n\nprint parsed messages in line delimited JSON format to stdout\n\n\n\n\n\n\nElasticsearch / Logsene\n\n\nLog storage\n\n\n\n\n\n\n-e, --elasticsearchUrl \n\n\nElasticsearch url e.g. http://localhost:9200, default htpps://logsene-receiver.sematext.com:443'\n\n\n\n\n\n\n-t, --index \n\n\nLogsene\n App Token to insert parsed records into Logsene or Elasticsearch index (see --elasticsearch-host)\n\n\n\n\n\n\n--httpProxy \n\n\nHTTP proxy url\n\n\n\n\n\n\n--httpsProxy \n\n\nHTTPS proxy url\n\n\n\n\n\n\nrtail\n\n\nRealtime log viewer\n\n\n\n\n\n\n--rtailPort\n\n\nforwards logs via UDP to \nrtail\n server\n\n\n\n\n\n\n--rtailHost hostname\n\n\nrtail\n server (UI for realtime logs), default: localhost\n\n\n\n\n\n\n--rtailWebPort \n\n\nstarts rtail UI webserver (if not installed install with: - npm i rtail -g)\n\n\n\n\n\n\n--rtailWebHost \n\n\nrtail UI webserver and bind hostname. E.g. \nlogagent --rtailWebPort 9000 --rtailPort 8989  --rtailWebHost $(hostname) -g \\'/var/log/**/*.log\n\n\n\n\n\n\n\n\nThe default output is line delimited JSON for parsed log lines, as long as no format options like -yaml (YAML format), -p (pretty JSON), or -s (silent, no output to console) are specified. \n\n\nEnvironment variables\n\n\n\n\n\n\n\n\nVariable\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nLOGSENE_TMP_DIR\n\n\nDirectory to store failed bulk requests, for later re-transmission.\n\n\n\n\n\n\nLOGSENE_LOG_INTERVAL\n\n\nTime to batch logs before a bulk request is done. Default 10000 ms (10 seconds)\n\n\n\n\n\n\nLOGSENE_BULK_SIZE\n\n\nMaximum size of a bulk request. Default 1000.\n\n\n\n\n\n\nLOGSENE_URL\n\n\nURL for the Logsene receiver. For a local Elasticsearch server or for On-Premise version of Logsene. Defaults to Sematext Logsene SaaS receiver https://logsene-receiver.sematext.com/_bulk. Example for Elasticsearch: \nLOGSENE_URL=http://localhost:9200/_bulk\n\n\n\n\n\n\nHTTPS_PROXY\n\n\nProxy URL for HTTPS endpoints, like Logsene receiver. \nexport HTTPS_PROXY=http://my-proxy.example\n\n\n\n\n\n\nHTTP_PROXY\n\n\nProxy URL for HTTP endpoints (e.g. On-Premises or local Elasticsearch). \nexport HTTP_PROXY=http://my-proxy.example\n\n\n\n\n\n\nLOGAGENT_CONFIG\n\n\nFilename to read logagent CLI parameters from a file, defaults to \n`/etc/sematext/logagent.conf\n\n\n\n\n\n\nPATTERN_MATCHING_ENABLED\n\n\nDefault is 'true'. The value 'false' disables parsing of logs.\n\n\n\n\n\n\n\n\nExamples\n\n\n# Be Evil: parse all logs \n# stream logs to Logsene 1-Click ELK stack \nlogagent -i LOGSENE_TOKEN /var/log/*.log \n# stream logs to local Elasticsearch  \nlogagent -e http://localhost:9200 -i myindex /var/log/*.log \n\n# Act as syslog server on UDP and forward messages to Logsene\nlogagent -u 514 -i LOGSENE_TOKEN  \n\n# Act as syslog server on UDP and write YAML formatted messages to console\nlogagent -u 514 -y  \n\n\n\n\nUse a \nglob\n pattern to build the file list \n\n\nlogagent -i LOGSENE_TOKEN -g '/var/log/**/*.log'\n# pass multiple glob patterns\nlogagent -i LOGSENE_TOKEN -g '{/var/log/*.log,/opt/myapp/*.log}'\n\n\n\n\nWatch selective log output on console by passing logs via stdin and format in YAML\n\n\ntail -f /var/log/access.log | logagent -y -n httpd\ntail -f /var/log/system.log | logagent -f my_own_patterns.yml  -y \n\n\n\n\nShip logs to rtail and Logsene to view logs in real-time in rtail and store logs in Logsene\n\n\n# rtail don't need to be installed, logagent uses the rtail protocol\nlogagent -i $LOGSENE_TOKEN --rtailHost myrtailserver --rtailPort 9999 /var/log/*.log\n\n\n\n\nLogagent can start the rtail web-server (in-process, saving memory), open browser with http://localhost:8080\n\n\n# logagent has no dependency to rtail, to keep the package small\nsudo npm i rtail -g\nlogagent -s -i $LOGSENE_TOKEN --rtailWebPort 8080 --rtailPort 9999 /var/log/*.log\n\n\n\n\nAnd of course you can combine rtail and Logagent in the traditional way, simply connect both via unix pipes. An example with rtail and Logsene storage and charts:", 
            "title": "Usage"
        }, 
        {
            "location": "/cli-parameters/#command-line-parameters", 
            "text": "", 
            "title": "Command Line Parameters"
        }, 
        {
            "location": "/cli-parameters/#synopsis", 
            "text": "logagent [options] [file list]     Options  Description      Genernal options     -h, --help  output logagent help    -V, --version  output logagent version    -v, --verbose  output activity report every minute    --config   path to logagent config file (see below)    --geoipEnabled   true/false to enable/disable geoip lookups in patterns.    --diskBufferDir  path  directory to store status and buffered logs (during network outage)    --includeOriginalLine  includes the original message in parsed logs    -f, --file   file with pattern definitions, use multiple -f options multiple files    -s, --suppress  silent, print no logs to stdout, prints only stats on exit    --printStats  print processing stats in the given interval in seconds, e.g.  --print_stats 30  to stderr. Usefull with -s to see logagent activity on the console without printing the parsed logs to stdout.    Log input options     -g glob-pattern  use a  glob  pattern to watch log files e.g.  -g \"{/var/log/*.log,/Users/stefan/myapp/*.log}\" . The complete glob expression must be quoted, to avoid interpretation of special characters by the linux shell.    +  --tailStartPosition bytes    --stdin  read from stdin, default if no other input like files or UDP are set    -n name  name for the log source only when stdin is used, important to make multi-line patterns working on stdin because the status is tracked by the log source name.    -u UDP_PORT  starts a syslogd UDP listener on the given port to act as syslogd    --heroku PORT  listens for Heroku logs (http drain / framed syslog over http)    --cfhttp PORT  listens for Cloud Foundry logs (syslog over http)    list of files  Every argument after the options list is interpreted as file name. All files in the file list (e.g. /var/log/*.log) are watched by  tail-forever  starting at end of file    Output options     standard output stream  combine logagent with any unix tool via pipes    -y, --yaml  prints parsed messages in YAML format to stdout    -p, --pretty  prints parsed messages in pretty json format to stdout    -j, --ldjson  print parsed messages in line delimited JSON format to stdout    Elasticsearch / Logsene  Log storage    -e, --elasticsearchUrl   Elasticsearch url e.g. http://localhost:9200, default htpps://logsene-receiver.sematext.com:443'    -t, --index   Logsene  App Token to insert parsed records into Logsene or Elasticsearch index (see --elasticsearch-host)    --httpProxy   HTTP proxy url    --httpsProxy   HTTPS proxy url    rtail  Realtime log viewer    --rtailPort  forwards logs via UDP to  rtail  server    --rtailHost hostname  rtail  server (UI for realtime logs), default: localhost    --rtailWebPort   starts rtail UI webserver (if not installed install with: - npm i rtail -g)    --rtailWebHost   rtail UI webserver and bind hostname. E.g.  logagent --rtailWebPort 9000 --rtailPort 8989  --rtailWebHost $(hostname) -g \\'/var/log/**/*.log     The default output is line delimited JSON for parsed log lines, as long as no format options like -yaml (YAML format), -p (pretty JSON), or -s (silent, no output to console) are specified.", 
            "title": "Synopsis"
        }, 
        {
            "location": "/cli-parameters/#environment-variables", 
            "text": "Variable  Description      LOGSENE_TMP_DIR  Directory to store failed bulk requests, for later re-transmission.    LOGSENE_LOG_INTERVAL  Time to batch logs before a bulk request is done. Default 10000 ms (10 seconds)    LOGSENE_BULK_SIZE  Maximum size of a bulk request. Default 1000.    LOGSENE_URL  URL for the Logsene receiver. For a local Elasticsearch server or for On-Premise version of Logsene. Defaults to Sematext Logsene SaaS receiver https://logsene-receiver.sematext.com/_bulk. Example for Elasticsearch:  LOGSENE_URL=http://localhost:9200/_bulk    HTTPS_PROXY  Proxy URL for HTTPS endpoints, like Logsene receiver.  export HTTPS_PROXY=http://my-proxy.example    HTTP_PROXY  Proxy URL for HTTP endpoints (e.g. On-Premises or local Elasticsearch).  export HTTP_PROXY=http://my-proxy.example    LOGAGENT_CONFIG  Filename to read logagent CLI parameters from a file, defaults to  `/etc/sematext/logagent.conf    PATTERN_MATCHING_ENABLED  Default is 'true'. The value 'false' disables parsing of logs.", 
            "title": "Environment variables"
        }, 
        {
            "location": "/cli-parameters/#examples", 
            "text": "# Be Evil: parse all logs \n# stream logs to Logsene 1-Click ELK stack \nlogagent -i LOGSENE_TOKEN /var/log/*.log \n# stream logs to local Elasticsearch  \nlogagent -e http://localhost:9200 -i myindex /var/log/*.log \n\n# Act as syslog server on UDP and forward messages to Logsene\nlogagent -u 514 -i LOGSENE_TOKEN  \n\n# Act as syslog server on UDP and write YAML formatted messages to console\nlogagent -u 514 -y    Use a  glob  pattern to build the file list   logagent -i LOGSENE_TOKEN -g '/var/log/**/*.log'\n# pass multiple glob patterns\nlogagent -i LOGSENE_TOKEN -g '{/var/log/*.log,/opt/myapp/*.log}'  Watch selective log output on console by passing logs via stdin and format in YAML  tail -f /var/log/access.log | logagent -y -n httpd\ntail -f /var/log/system.log | logagent -f my_own_patterns.yml  -y   Ship logs to rtail and Logsene to view logs in real-time in rtail and store logs in Logsene  # rtail don't need to be installed, logagent uses the rtail protocol\nlogagent -i $LOGSENE_TOKEN --rtailHost myrtailserver --rtailPort 9999 /var/log/*.log  Logagent can start the rtail web-server (in-process, saving memory), open browser with http://localhost:8080  # logagent has no dependency to rtail, to keep the package small\nsudo npm i rtail -g\nlogagent -s -i $LOGSENE_TOKEN --rtailWebPort 8080 --rtailPort 9999 /var/log/*.log  And of course you can combine rtail and Logagent in the traditional way, simply connect both via unix pipes. An example with rtail and Logsene storage and charts:", 
            "title": "Examples"
        }, 
        {
            "location": "/config-file/", 
            "text": "Config File\n\n\nLogagent can be configured via config files in YAML format. \nTo use the config file run:\n\n\nlogagent --config configFileName.yml\n\n\n\n\nWhen logagent is istalled as \nsystem service\n the default config file is located in \n\n\n/etc/sematext/logagent.conf\n\n\n\n\nSection: options\n\n\n# Global options\noptions:\n  # print stats every 60 seconds \n  printStats: 60\n  # don't write parsed logs to stdout\n  suppress: false\n  # Enalbe/disable GeoIP lookups\n  # Startup of logagent might be slower, when downloading the GeoIP database\n  geoipEnabled: false\n  # Directory to store Logagent status nad temporary files\n  diskBufferDir: ./tmp\n\n\n\n\nSection: input\n\n\ninput:\n  # a list of glob patterns to watch files to tail\n  files:\n      - '/var/log/**/*.log'\n      - '/opt/myapp/logs/*.log'\n  # listen to udp syslog protocol  \n  #syslog: \n  #  port: 514\n  # listen to http to receive data from Heroku  log drains  \n  #heroku: \n  #  port: 9999\n  # listen to http to receive data from Cloud Foundry drains  \n  #cloudFoundry:\n  #  port: 8888\n\n\n\n\nSection: parser\n\n\nIn this section defines loading of custom pattern files or inline pattern definitions for the log parser.\n\n\n# optional, if not specified default patterns are used\nparser:\n  patternFiles:\n    # load a list of pattern files to parse logs\n    # later files overwrite settings from previous files\n    # a 'hot reload' is done as soon one of the listed fiels changes on disk\n    - patterns1.yml\n    - patterns2.yml\n  # inline pattern definitions, to put on top of patterns list\n  # loaded from files or default librarary. For inline patterns hot reload is not available.  \n  patterns:\n    - # timestamped messages from /var/log/*.log on Mac OS X\n      sourceName: !!js/regexp /\\system\\.log/ # catch all system.log files  \n      match:\n        -\n          type: system_log\n          regex: !!js/regexp /([\\w|\\s]+\\s+\\d{2}\\s[\\d|\\:]+)\\s(.+?)\\s(.+?)\\s\n(.+)\n(.*)/\n          fields: [ts,host,service,severity,message]\n          dateFormat: MMM DD HH:mm:ss\n\n\n\n\nSection: output\n\n\nLogs could be shipped to Elasticsearch or to rtail for realtime log view. \nThe Elasticsearch output supports HTTPS, username/password in the url. \nIn addtion it is possible to route logs from different files to different indicies in Elasticsearch. All logs, which don't match the rules in the indices section are routed to the default index (elasticsearch.index). \n\n\noutput:\n  # index logs in Elasticsearch or Logsene\n  elasticsearch: \n    # URL to Elasticearch server, defaults to Logsene SaaS if not set\n    url: https://logsene-receiver.sematext.com\n\n    # Proxy settings behind firewalls\n    # httpProxy:  http://localProxy:port\n    # httpsProxy: https://localHttpsProxy:port\n\n    # default index to use, for all logs that don't match later in indices section\n    # for Logsene use the Logsene App Token here\n    index: 0a835c75-9847-4f74-xxxx\n\n    # specific index to use per logSource field of parsed logs\n    # logSource is by default the file name of the log file\n    # but it can be modified by JS transforms functions in the patterns.yml file\n    indices: \n      4f70a0c7-9458-43e2-bbc5-xxxx: \n      # list of RegEx mathich logSource / filename  \n      # all logs matching logSource name will be indexed to above index\n        - .*wifi.*\n        - .*bluetooth.*\n      999532c9-18f1-4c4b-8753-xxxx: \n        - system\\.log\n        - access\\.log\n        - auth\\.log\n  # print parsed logs in YAML format to stdout (only if options.supress is set to false)    \n  stdout: yaml # use 'pretty' for pretty json and 'ldjson' for line delimited json (default)\n\n  # forward logs to rtail realtime log viewer\n  #rtail:\n    # rtail host to send logs to\n    #host: localhost\n    # rtails port to send logs to \n    #udpPort: 3434\n    # start rtail Server with given http port and bind to address of hostname\n    #webPort: 8080\n    #webHost: localhost\n\n\n\n\nA collection of exmaple config files are \nhere", 
            "title": "Configuration file"
        }, 
        {
            "location": "/config-file/#config-file", 
            "text": "Logagent can be configured via config files in YAML format. \nTo use the config file run:  logagent --config configFileName.yml  When logagent is istalled as  system service  the default config file is located in   /etc/sematext/logagent.conf", 
            "title": "Config File"
        }, 
        {
            "location": "/config-file/#section-options", 
            "text": "# Global options\noptions:\n  # print stats every 60 seconds \n  printStats: 60\n  # don't write parsed logs to stdout\n  suppress: false\n  # Enalbe/disable GeoIP lookups\n  # Startup of logagent might be slower, when downloading the GeoIP database\n  geoipEnabled: false\n  # Directory to store Logagent status nad temporary files\n  diskBufferDir: ./tmp", 
            "title": "Section: options"
        }, 
        {
            "location": "/config-file/#section-input", 
            "text": "input:\n  # a list of glob patterns to watch files to tail\n  files:\n      - '/var/log/**/*.log'\n      - '/opt/myapp/logs/*.log'\n  # listen to udp syslog protocol  \n  #syslog: \n  #  port: 514\n  # listen to http to receive data from Heroku  log drains  \n  #heroku: \n  #  port: 9999\n  # listen to http to receive data from Cloud Foundry drains  \n  #cloudFoundry:\n  #  port: 8888", 
            "title": "Section: input"
        }, 
        {
            "location": "/config-file/#section-parser", 
            "text": "In this section defines loading of custom pattern files or inline pattern definitions for the log parser.  # optional, if not specified default patterns are used\nparser:\n  patternFiles:\n    # load a list of pattern files to parse logs\n    # later files overwrite settings from previous files\n    # a 'hot reload' is done as soon one of the listed fiels changes on disk\n    - patterns1.yml\n    - patterns2.yml\n  # inline pattern definitions, to put on top of patterns list\n  # loaded from files or default librarary. For inline patterns hot reload is not available.  \n  patterns:\n    - # timestamped messages from /var/log/*.log on Mac OS X\n      sourceName: !!js/regexp /\\system\\.log/ # catch all system.log files  \n      match:\n        -\n          type: system_log\n          regex: !!js/regexp /([\\w|\\s]+\\s+\\d{2}\\s[\\d|\\:]+)\\s(.+?)\\s(.+?)\\s (.+) (.*)/\n          fields: [ts,host,service,severity,message]\n          dateFormat: MMM DD HH:mm:ss", 
            "title": "Section: parser"
        }, 
        {
            "location": "/config-file/#section-output", 
            "text": "Logs could be shipped to Elasticsearch or to rtail for realtime log view. \nThe Elasticsearch output supports HTTPS, username/password in the url. \nIn addtion it is possible to route logs from different files to different indicies in Elasticsearch. All logs, which don't match the rules in the indices section are routed to the default index (elasticsearch.index).   output:\n  # index logs in Elasticsearch or Logsene\n  elasticsearch: \n    # URL to Elasticearch server, defaults to Logsene SaaS if not set\n    url: https://logsene-receiver.sematext.com\n\n    # Proxy settings behind firewalls\n    # httpProxy:  http://localProxy:port\n    # httpsProxy: https://localHttpsProxy:port\n\n    # default index to use, for all logs that don't match later in indices section\n    # for Logsene use the Logsene App Token here\n    index: 0a835c75-9847-4f74-xxxx\n\n    # specific index to use per logSource field of parsed logs\n    # logSource is by default the file name of the log file\n    # but it can be modified by JS transforms functions in the patterns.yml file\n    indices: \n      4f70a0c7-9458-43e2-bbc5-xxxx: \n      # list of RegEx mathich logSource / filename  \n      # all logs matching logSource name will be indexed to above index\n        - .*wifi.*\n        - .*bluetooth.*\n      999532c9-18f1-4c4b-8753-xxxx: \n        - system\\.log\n        - access\\.log\n        - auth\\.log\n  # print parsed logs in YAML format to stdout (only if options.supress is set to false)    \n  stdout: yaml # use 'pretty' for pretty json and 'ldjson' for line delimited json (default)\n\n  # forward logs to rtail realtime log viewer\n  #rtail:\n    # rtail host to send logs to\n    #host: localhost\n    # rtails port to send logs to \n    #udpPort: 3434\n    # start rtail Server with given http port and bind to address of hostname\n    #webPort: 8080\n    #webHost: localhost  A collection of exmaple config files are  here", 
            "title": "Section: output"
        }, 
        {
            "location": "/parser/", 
            "text": "How does the parser work?\n\n\nThe parser detects log formats based on a pattern library (yaml file) and converts it to a JSON Object:\n\n\n\n\nfind matching regex in pattern library\n\n\ntag it with the recognized type\n\n\nextract fields using regex\n\n\nif 'autohash' is enabled, sensitive data is replaced with its sha1 hash code\n\n\nparse dates and detect date format\n  (use 'ts' field for date and time combined) \n\n\ncreate ISO timestamp in '@timestamp' field\n\n\ntransform function to manipulate parsed objects\n\n\nunmatched lines end up with timestamp and original line in the message field\n\n\nJSON lines are parsed, and scanned for @timestamp and time fields (logstash and bunyan format)\n\n\ndefault patterns for many applications (see below)\n\n\nHeroku logs\n\n\n\n\nThe default pattern definition file comes with patterns for:\n\n\n\n\nMongoDB\n\n\nMySQL\n\n\nNginx\n\n\nRedis\n\n\nElasticsearch\n\n\nWebserver (nginx, apache httpd)\n\n\nZookeeper\n\n\nCassandra\n\n\nKafka\n\n\nHBase HDFS Data Node\n\n\nHBase Region Server\n\n\nHadoop YARN Node Manager\n\n\nApache Solr\n\n\nvarious Linux/Mac OS X system log files\n\n\n\n\nThe file format is based on \nJS-YAML\n, in short:\n\n\n- - indicates an  array\n- !js/regexp - indicates a JS regular expression\n- !!js/function \n - indicates a JS function \n\n\n\n\nProperties:\n\n\n\n\npatterns: list of patterns, each pattern starts with \"-\"\n\n\nmatch: group of patterns for a specific log source\n\n\nregex: JS regular expression \n\n\nfields: field list of extracted match groups from the regex\n\n\ntype: type used in Logsene (Elasticsearch Mapping)\n\n\ndateFormat: format of the special fields 'ts', if the date format matches, a new field @timestamp is generated\n\n\ntransform: JS function to manipulate the result of regex and date parsing\n\n\n\n\nExample\n\n\n# Sensitive data can be replaced with a hashcode (sha1)\n# it applies to fields matching the field names by a regular expression\n# Note: this function is not optimized (yet) and might take 10-15% of performance\nautohash: !!js/regexp /user|password|email|credit_card_number|payment_info/i\n\n# set this to false when autohash fields\n# the original line might include sensitive data!\noriginalLine: false\n\n# activate GeoIP lookup\ngeoIP: true\n\n# logagent updates geoip db files automatically\n# pls. note write access to this directory is required\nmaxmindDbDir: /tmp/\n\npatterns: \n  - # APACHE  Web Logs\n  sourceName: httpd\n  match: \n    # Common Log Format\n    - regex:        !!js/regexp /([0-9a-f.:]+)\\s+(-|.+?)\\s+(-|.+?)\\s+\\[([0-9]{2}\\/[a-z]{3}\\/[0-9]{4}\\:[0-9]{2}:[0-9]{2}:[0-9]{2}[^\\]]*)\\] \\\n(\\S+?)\\s(\\S*?)\\s{0,1}(\\S+?)\\\n ([0-9|\\-]+) ([0-9|\\-]+)/i\n      type: apache_access_common\n      fields:       [client_ip,remote_id,user,ts,method,path,http_version,status_code,size]\n      dateFormat: DD/MMM/YYYY:HH:mm:ss ZZ\n      # lookup geoip info for the field client_ip\n      geoIP: client_ip\n      # parse only messages that match this regex\n      inputFilter: !!js/regexp /api|home|user/\n      # ignore messages matching inputDrop\n      inputDrop: !!js/regexp /127.0.0.1|\\.css|\\.js|\\.png|\\.jpg|\\.jpeg/\n      # modify parsed object\n      transform: !!js/function \n\n        function (p) {\n          p.message = p.method + ' ' + p.path\n        }\n      customPropertyMinStatusCode: 399\n      filter: !!js/function \n \n        function (p, pattern) {\n          // log only requests with status code \n 399\n          return p.status_code \n pattern.customPropertyMinStatusCode // 399\n        }\n\n\n\n\nThe handling of JSON is different, regular expressions are not matched against JSON data. \nLogagent parse JSON and provides post processing functions in the pattern definition.\nThe following example masks fields in JSON and removes fields from the parsed event. \n\n\nhashFunction: sha512\n# post process journald JSON format\n# logagent feature to hash fields\n# and a custom property 'removeFields', used in the transform function\njson: \n  autohashFields: \n    - _HOSTNAME: true\n  removeFields: \n    - _SOURCE_REALTIME_TIMESTAMP\n    - __MONOTONIC_TIMESTAMP\n  transform: !!js/function \n\n   function (source, parsedObject, config) {\n     for (var i=0; i\nconfig.removeFields.length; i++) {\n       // console.log('delete ' +config.removeFields[i])\n       delete parsedObject[config.removeFields[i]]\n     }\n   }\n\n\n\n\nThe default patterns are available \nhere\n - contributions are welcome!\n\n\nNode.js API for the parser\n\n\nInstall logagent as local module and save the dependency to your package.json\n\n\nnpm i logagent-js --save\n\n\n\n\nUse the Logparser module in your source code\n\n\nvar Logparser = require('logagent-js')\nvar lp = new Logparser('./patterns.yml')\nlp.parseLine('log message', 'source name', function (err, data) {\n    if(err) {\n      console.log('line did not match any pattern')\n    }\n    console.log(JSON.stringify(data))\n})\n\n\n\n\nTo test patterns or convert logs from text to JSON use the command line tool 'logagent'. It reads from stdin and outputs line delimited JSON (or pretty JSON or YAML) to the console. In addition, it can forward the parsed objects directly to \nLogsene\n or Elasticsearch.\n\n\nTest your patterns:\n\n\ncat myapp.log | bin/logagent -y -n myapp -f mypatterns.yml", 
            "title": "Log Parser and pattern definitions"
        }, 
        {
            "location": "/parser/#how-does-the-parser-work", 
            "text": "The parser detects log formats based on a pattern library (yaml file) and converts it to a JSON Object:   find matching regex in pattern library  tag it with the recognized type  extract fields using regex  if 'autohash' is enabled, sensitive data is replaced with its sha1 hash code  parse dates and detect date format\n  (use 'ts' field for date and time combined)   create ISO timestamp in '@timestamp' field  transform function to manipulate parsed objects  unmatched lines end up with timestamp and original line in the message field  JSON lines are parsed, and scanned for @timestamp and time fields (logstash and bunyan format)  default patterns for many applications (see below)  Heroku logs   The default pattern definition file comes with patterns for:   MongoDB  MySQL  Nginx  Redis  Elasticsearch  Webserver (nginx, apache httpd)  Zookeeper  Cassandra  Kafka  HBase HDFS Data Node  HBase Region Server  Hadoop YARN Node Manager  Apache Solr  various Linux/Mac OS X system log files   The file format is based on  JS-YAML , in short:  - - indicates an  array\n- !js/regexp - indicates a JS regular expression\n- !!js/function   - indicates a JS function   Properties:   patterns: list of patterns, each pattern starts with \"-\"  match: group of patterns for a specific log source  regex: JS regular expression   fields: field list of extracted match groups from the regex  type: type used in Logsene (Elasticsearch Mapping)  dateFormat: format of the special fields 'ts', if the date format matches, a new field @timestamp is generated  transform: JS function to manipulate the result of regex and date parsing", 
            "title": "How does the parser work?"
        }, 
        {
            "location": "/parser/#example", 
            "text": "# Sensitive data can be replaced with a hashcode (sha1)\n# it applies to fields matching the field names by a regular expression\n# Note: this function is not optimized (yet) and might take 10-15% of performance\nautohash: !!js/regexp /user|password|email|credit_card_number|payment_info/i\n\n# set this to false when autohash fields\n# the original line might include sensitive data!\noriginalLine: false\n\n# activate GeoIP lookup\ngeoIP: true\n\n# logagent updates geoip db files automatically\n# pls. note write access to this directory is required\nmaxmindDbDir: /tmp/\n\npatterns: \n  - # APACHE  Web Logs\n  sourceName: httpd\n  match: \n    # Common Log Format\n    - regex:        !!js/regexp /([0-9a-f.:]+)\\s+(-|.+?)\\s+(-|.+?)\\s+\\[([0-9]{2}\\/[a-z]{3}\\/[0-9]{4}\\:[0-9]{2}:[0-9]{2}:[0-9]{2}[^\\]]*)\\] \\ (\\S+?)\\s(\\S*?)\\s{0,1}(\\S+?)\\  ([0-9|\\-]+) ([0-9|\\-]+)/i\n      type: apache_access_common\n      fields:       [client_ip,remote_id,user,ts,method,path,http_version,status_code,size]\n      dateFormat: DD/MMM/YYYY:HH:mm:ss ZZ\n      # lookup geoip info for the field client_ip\n      geoIP: client_ip\n      # parse only messages that match this regex\n      inputFilter: !!js/regexp /api|home|user/\n      # ignore messages matching inputDrop\n      inputDrop: !!js/regexp /127.0.0.1|\\.css|\\.js|\\.png|\\.jpg|\\.jpeg/\n      # modify parsed object\n      transform: !!js/function  \n        function (p) {\n          p.message = p.method + ' ' + p.path\n        }\n      customPropertyMinStatusCode: 399\n      filter: !!js/function   \n        function (p, pattern) {\n          // log only requests with status code   399\n          return p.status_code   pattern.customPropertyMinStatusCode // 399\n        }  The handling of JSON is different, regular expressions are not matched against JSON data. \nLogagent parse JSON and provides post processing functions in the pattern definition.\nThe following example masks fields in JSON and removes fields from the parsed event.   hashFunction: sha512\n# post process journald JSON format\n# logagent feature to hash fields\n# and a custom property 'removeFields', used in the transform function\njson: \n  autohashFields: \n    - _HOSTNAME: true\n  removeFields: \n    - _SOURCE_REALTIME_TIMESTAMP\n    - __MONOTONIC_TIMESTAMP\n  transform: !!js/function  \n   function (source, parsedObject, config) {\n     for (var i=0; i config.removeFields.length; i++) {\n       // console.log('delete ' +config.removeFields[i])\n       delete parsedObject[config.removeFields[i]]\n     }\n   }  The default patterns are available  here  - contributions are welcome!", 
            "title": "Example"
        }, 
        {
            "location": "/parser/#nodejs-api-for-the-parser", 
            "text": "Install logagent as local module and save the dependency to your package.json  npm i logagent-js --save  Use the Logparser module in your source code  var Logparser = require('logagent-js')\nvar lp = new Logparser('./patterns.yml')\nlp.parseLine('log message', 'source name', function (err, data) {\n    if(err) {\n      console.log('line did not match any pattern')\n    }\n    console.log(JSON.stringify(data))\n})  To test patterns or convert logs from text to JSON use the command line tool 'logagent'. It reads from stdin and outputs line delimited JSON (or pretty JSON or YAML) to the console. In addition, it can forward the parsed objects directly to  Logsene  or Elasticsearch.  Test your patterns:  cat myapp.log | bin/logagent -y -n myapp -f mypatterns.yml", 
            "title": "Node.js API for the parser"
        }, 
        {
            "location": "/plugins/", 
            "text": "Logagent plugins\n\n\nThe architecture of logagent is modular and each input or output module is implemented as plugin for the logagent framework. Plugins are loaded on demand depending on the configurations. \n\n\nHow plugins work\n\n\n\n\nLogagent checks the configuration file for properties with a \"module\" key for the nodejs module name. External plugins needs to be installed via npm. \n\n\nPlugins are initialized with the logagent configuration from (command line arguments + configuration file) and the event emitter for logagent. Plugins should provide a start and stop method.\n\n\nInput plugins read data from a data source and emit events to logagent event emitter.\n  This events have the identifier \"data.raw\" and 2 parameters: \n\n\ndata - data read from a data source \n\n\ncontext - an object with meta data e.g. {sourceName: '/var/log/httpd/access.log'}\n    The \"context\" helps other plugins to process the data coreectly, e.g. to handle multiple open files. \n\n\nOutput plugins listen to \"data.parsed\" events and store or forward the data to the target. \n\n\n\n\nExamples\n\n\nInput plugin\n\n\nThis example\n implements a plugin to receive data via TCP socket with a configurable rate limit. \n\n\nThe plugin config file: \n\n\n# Global options\ninput:\n  tcp: \n    module: @sematext/logagent-tcp-input\n    port: 45900\n    bindAddress: 0.0.0.0\n    sourceName: tcpTest\noutput:\n  # print parsed logs in YAML format to stdout   \n  stdout: yaml \n\n\n\n\nNode.js source code:\n\n\n'use strict'\nvar split = require('split2')\nvar net = require('net')\nvar safeStringify = require('fast-safe-stringify')\n\n/**\n * Constructor called by logagent, when the config file contains tis entry: \n * input\n *  tcp:\n *    module: megastef/logagent-input-tcp\n *    port: 4545\n *    bindAddress: 0.0.0.0\n *\n * @config cli arguments and config.configFile entries\n * @eventEmitter logent eventEmitter object\n */\nfunction InputTCP (config, eventEmitter) {\n  this.config = config.configFile.input.tcp\n  this.config.maxInputRate = config.configFile.input.tcp.maxInputRate || config.maxInputRate\n  this.eventEmitter = eventEmitter\n}\nmodule.exports = InputTCP\n/**\n * Plugin start function, called after constructor\n *\n */\nInputTCP.prototype.start = function () {\n  if (!this.started) {\n    this.createServer()\n    this.started = true\n  }\n}\n\n/**\n * Plugin stop function, called when logagent terminates\n * we close the server socket here.\n */\nInputTCP.prototype.stop = function (cb) {\n  this.server.close(cb)\n}\n\nInputTCP.prototype.createServer = function () {\n  var self = this\n  this.server = net.createServer(function (socket) {\n    // Context object, the source name is used to identify patterns\n    var context = { name: 'input.tcp', sourceName: self.config.sourceName || socket.remoteAddress + ':' + socket.remotePort }\n    socket.pipe(Throttle(self.config.maxInputRate)).pipe(split()).on('data', function emitLine (data) {\n      // emit a 'data.raw' event for each line we receive\n      self.eventEmitter.emit('data.raw', data, context)\n      if (self.config.debug) {\n        console.log(data, context)\n      }\n    }).on('error', console.error)\n  /*\n  // We could return parsed objects to the client\n  // Logagent will emit \ndata.parsed\n events\n  self.eventEmitter.on('data.parsed', function (data, aContext) {\n    socket.write(safeStringify(data) + '\\n')\n  })\n  */\n  })\n  var port = this.config.port || 4545\n  var address = this.config.bindAddress || '0. 0.0.0'\n  this.server.listen(port, address)\n  console.log('listening to ' + address + ':' + port)\n}\n\n// helper  to throttle bandwidth\nvar StreamThrottle = require('stream-throttle').Throttle\nfunction Throttle (maxRate) {\n  var inputRate = maxRate || 1024 * 1024 * 100\n  var chunkSize = inputRate / 10\n  if (chunkSize \n 1) {\n    chunkSize = 1\n  }\n  return new StreamThrottle({\n    chunksize: chunkSize,\n    rate: inputRate || 1024 * 1024 * 100\n  })\n}\n\n\n\n\nExample output plugin\n\n\n'use strict'\nvar prettyjson = require('prettyjson')\nvar safeStringify = require('fast-safe-stringify')\nfunction OutputStdout (config, eventEmitter) {\n  this.config = config\n  this.eventEmitter = eventEmitter\n}\n\nOutputStdout.prototype.eventHandler = function (data, context) {\n  if (this.config.suppress) {\n    return\n  }\n  if (this.config.pretty) {\n    console.log(JSON.stringify(data, null, '\\t'))\n  } else if (this.config.yaml) {\n    console.log(prettyjson.render(data, {noColor: false}) + '\\n')\n  } else {\n    console.log(safeStringify(data))\n  }\n}\n\nOutputStdout.prototype.start = function () {\n  this.eventEmitter.on('data.parsed', this.eventHandler.bind(this))\n}\n\nOutputStdout.prototype.stop = function (cb) {\n  this.eventEmitter.removeListener('data.parsed', this.eventHandler)\n  cb()\n}\n\nmodule.exports = OutputStdout", 
            "title": "Overview"
        }, 
        {
            "location": "/plugins/#logagent-plugins", 
            "text": "The architecture of logagent is modular and each input or output module is implemented as plugin for the logagent framework. Plugins are loaded on demand depending on the configurations.", 
            "title": "Logagent plugins"
        }, 
        {
            "location": "/plugins/#how-plugins-work", 
            "text": "Logagent checks the configuration file for properties with a \"module\" key for the nodejs module name. External plugins needs to be installed via npm.   Plugins are initialized with the logagent configuration from (command line arguments + configuration file) and the event emitter for logagent. Plugins should provide a start and stop method.  Input plugins read data from a data source and emit events to logagent event emitter.\n  This events have the identifier \"data.raw\" and 2 parameters:   data - data read from a data source   context - an object with meta data e.g. {sourceName: '/var/log/httpd/access.log'}\n    The \"context\" helps other plugins to process the data coreectly, e.g. to handle multiple open files.   Output plugins listen to \"data.parsed\" events and store or forward the data to the target.", 
            "title": "How plugins work"
        }, 
        {
            "location": "/plugins/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/plugins/#input-plugin", 
            "text": "This example  implements a plugin to receive data via TCP socket with a configurable rate limit.   The plugin config file:   # Global options\ninput:\n  tcp: \n    module: @sematext/logagent-tcp-input\n    port: 45900\n    bindAddress: 0.0.0.0\n    sourceName: tcpTest\noutput:\n  # print parsed logs in YAML format to stdout   \n  stdout: yaml   Node.js source code:  'use strict'\nvar split = require('split2')\nvar net = require('net')\nvar safeStringify = require('fast-safe-stringify')\n\n/**\n * Constructor called by logagent, when the config file contains tis entry: \n * input\n *  tcp:\n *    module: megastef/logagent-input-tcp\n *    port: 4545\n *    bindAddress: 0.0.0.0\n *\n * @config cli arguments and config.configFile entries\n * @eventEmitter logent eventEmitter object\n */\nfunction InputTCP (config, eventEmitter) {\n  this.config = config.configFile.input.tcp\n  this.config.maxInputRate = config.configFile.input.tcp.maxInputRate || config.maxInputRate\n  this.eventEmitter = eventEmitter\n}\nmodule.exports = InputTCP\n/**\n * Plugin start function, called after constructor\n *\n */\nInputTCP.prototype.start = function () {\n  if (!this.started) {\n    this.createServer()\n    this.started = true\n  }\n}\n\n/**\n * Plugin stop function, called when logagent terminates\n * we close the server socket here.\n */\nInputTCP.prototype.stop = function (cb) {\n  this.server.close(cb)\n}\n\nInputTCP.prototype.createServer = function () {\n  var self = this\n  this.server = net.createServer(function (socket) {\n    // Context object, the source name is used to identify patterns\n    var context = { name: 'input.tcp', sourceName: self.config.sourceName || socket.remoteAddress + ':' + socket.remotePort }\n    socket.pipe(Throttle(self.config.maxInputRate)).pipe(split()).on('data', function emitLine (data) {\n      // emit a 'data.raw' event for each line we receive\n      self.eventEmitter.emit('data.raw', data, context)\n      if (self.config.debug) {\n        console.log(data, context)\n      }\n    }).on('error', console.error)\n  /*\n  // We could return parsed objects to the client\n  // Logagent will emit  data.parsed  events\n  self.eventEmitter.on('data.parsed', function (data, aContext) {\n    socket.write(safeStringify(data) + '\\n')\n  })\n  */\n  })\n  var port = this.config.port || 4545\n  var address = this.config.bindAddress || '0. 0.0.0'\n  this.server.listen(port, address)\n  console.log('listening to ' + address + ':' + port)\n}\n\n// helper  to throttle bandwidth\nvar StreamThrottle = require('stream-throttle').Throttle\nfunction Throttle (maxRate) {\n  var inputRate = maxRate || 1024 * 1024 * 100\n  var chunkSize = inputRate / 10\n  if (chunkSize   1) {\n    chunkSize = 1\n  }\n  return new StreamThrottle({\n    chunksize: chunkSize,\n    rate: inputRate || 1024 * 1024 * 100\n  })\n}", 
            "title": "Input plugin"
        }, 
        {
            "location": "/plugins/#example-output-plugin", 
            "text": "'use strict'\nvar prettyjson = require('prettyjson')\nvar safeStringify = require('fast-safe-stringify')\nfunction OutputStdout (config, eventEmitter) {\n  this.config = config\n  this.eventEmitter = eventEmitter\n}\n\nOutputStdout.prototype.eventHandler = function (data, context) {\n  if (this.config.suppress) {\n    return\n  }\n  if (this.config.pretty) {\n    console.log(JSON.stringify(data, null, '\\t'))\n  } else if (this.config.yaml) {\n    console.log(prettyjson.render(data, {noColor: false}) + '\\n')\n  } else {\n    console.log(safeStringify(data))\n  }\n}\n\nOutputStdout.prototype.start = function () {\n  this.eventEmitter.on('data.parsed', this.eventHandler.bind(this))\n}\n\nOutputStdout.prototype.stop = function (cb) {\n  this.eventEmitter.removeListener('data.parsed', this.eventHandler)\n  cb()\n}\n\nmodule.exports = OutputStdout", 
            "title": "Example output plugin"
        }, 
        {
            "location": "/output-elasticsearch/", 
            "text": "Elasticsearch Output Plugin\n\n\nThe Elasticsearch output plugin forwards parsed logs to \nElasticsearch\n or \nLogsene\n. \n\n\nFeatures\n\n\n\n\nlog routing by log source to multiple Elasticsearch servers\n\n\nlog routing by log source to multiple Elasticearch indices (or Logsene Apps)\n\n\nSSL/TLS by default, when using Logsene\n\n\nTwo-way SSL Authentication, also known as Mutual Authentication as part of PKI, secure client authentication with SSL client certificates\n\n\nbulk indexing with timeout (1000 docs or 10 second timeout by default)\n\n\ndisk buffer an re-transmit when connection to Elasticsearch fails\n\n\nrenaming of invalid field names\n\n\nlimit field size (240k by default)\n\n\n\n\nSimple config\n\n\nThe following example configuration ships all log files in /var/log (including sub-directories) to one Elasticsearch index. \n\n\ninput:\n  files:\n      - '/var/log/**/*.log'\noutput:\n  my-logsene-app: \n    module: elasticsearch\n    url: https://logsene-receiver.sematext.com \n    index: bb308f80-0453-485e-894c-f80c054a0f10 \n\n\n\n\nLog routing to multiple targets\n\n\nIn some situations it is required to ship data from different sources to differnt Elasticsearch servers. The output section in the Logagnet configuration file accepts multiple definitions for the Elasticsearch output module. \n\n\nEach Elasticsearch output might have a a list list of indices followed by a list of regular expressions matching the log source (e.g. file name of the log file). \n\n\nThe following expamle ships logs from wireless devices and authentication logs to a local Elasticsearch server and other server logs to multiple Logsene applications. \n\n\ninput:\n  files:\n      - '/var/log/**/*.log'\n\noutput:\n  # index logs in Elasticsearch or Logsene\n  local-elasticsearch: \n    modul: elasticsearch\n    url: http://localhost:9200\n    # default index to use, for all logs that don't match any other configuration\n    index: other_logs\n    # specific indices to use per logSource field of parsed logs\n    indices: \n      wireless_logs: # use regex to match log source e.g. /var/log/wifi.log\n        - wifi|bluetooth\n      security_logs: \n        - auth\\.log\n   logsene-saas:\n     module: elasticsearch\n     url: https://logsene-receiver.sematext.com\n     indices:\n       bb308f80-0453-485e-894c-f80c054a0f10:\n        - [nginx|httpd]\\.log\n       a0ca5032-62da-467d-b6d5-e465a7ce45bb\n        - mysql|postgres|oracle\n       969020b4-f11c-41dd-86e4-24e67759cdb3\n        - mongo.*\\.log\n        - myapp1\\/app.log\n        - myapp2\\/app.log\n\n\n\n\nHTTP and HTTPS options\n\n\nThe Elasticsearch output module accepts \nhttp(s) options\n. Client side certifictes and keys are specified with a file name. If you use self signed certifictes, set  \nrejectUnauthorized\n to \nfalse\n.\n\n\noutput:\n  secure-elasticsearch: \n    module: elasticsearch\n    url: https://localhost \n    index: logs \n    httpOptions:\n      key: /ssl-keys/client.key\n      cert: /ssl-keys/client.crt\n      rejectUnauthorized: true", 
            "title": "Elasticsearch"
        }, 
        {
            "location": "/output-elasticsearch/#elasticsearch-output-plugin", 
            "text": "The Elasticsearch output plugin forwards parsed logs to  Elasticsearch  or  Logsene .", 
            "title": "Elasticsearch Output Plugin"
        }, 
        {
            "location": "/output-elasticsearch/#features", 
            "text": "log routing by log source to multiple Elasticsearch servers  log routing by log source to multiple Elasticearch indices (or Logsene Apps)  SSL/TLS by default, when using Logsene  Two-way SSL Authentication, also known as Mutual Authentication as part of PKI, secure client authentication with SSL client certificates  bulk indexing with timeout (1000 docs or 10 second timeout by default)  disk buffer an re-transmit when connection to Elasticsearch fails  renaming of invalid field names  limit field size (240k by default)", 
            "title": "Features"
        }, 
        {
            "location": "/output-elasticsearch/#simple-config", 
            "text": "The following example configuration ships all log files in /var/log (including sub-directories) to one Elasticsearch index.   input:\n  files:\n      - '/var/log/**/*.log'\noutput:\n  my-logsene-app: \n    module: elasticsearch\n    url: https://logsene-receiver.sematext.com \n    index: bb308f80-0453-485e-894c-f80c054a0f10", 
            "title": "Simple config"
        }, 
        {
            "location": "/output-elasticsearch/#log-routing-to-multiple-targets", 
            "text": "In some situations it is required to ship data from different sources to differnt Elasticsearch servers. The output section in the Logagnet configuration file accepts multiple definitions for the Elasticsearch output module.   Each Elasticsearch output might have a a list list of indices followed by a list of regular expressions matching the log source (e.g. file name of the log file).   The following expamle ships logs from wireless devices and authentication logs to a local Elasticsearch server and other server logs to multiple Logsene applications.   input:\n  files:\n      - '/var/log/**/*.log'\n\noutput:\n  # index logs in Elasticsearch or Logsene\n  local-elasticsearch: \n    modul: elasticsearch\n    url: http://localhost:9200\n    # default index to use, for all logs that don't match any other configuration\n    index: other_logs\n    # specific indices to use per logSource field of parsed logs\n    indices: \n      wireless_logs: # use regex to match log source e.g. /var/log/wifi.log\n        - wifi|bluetooth\n      security_logs: \n        - auth\\.log\n   logsene-saas:\n     module: elasticsearch\n     url: https://logsene-receiver.sematext.com\n     indices:\n       bb308f80-0453-485e-894c-f80c054a0f10:\n        - [nginx|httpd]\\.log\n       a0ca5032-62da-467d-b6d5-e465a7ce45bb\n        - mysql|postgres|oracle\n       969020b4-f11c-41dd-86e4-24e67759cdb3\n        - mongo.*\\.log\n        - myapp1\\/app.log\n        - myapp2\\/app.log", 
            "title": "Log routing to multiple targets"
        }, 
        {
            "location": "/output-elasticsearch/#http-and-https-options", 
            "text": "The Elasticsearch output module accepts  http(s) options . Client side certifictes and keys are specified with a file name. If you use self signed certifictes, set   rejectUnauthorized  to  false .  output:\n  secure-elasticsearch: \n    module: elasticsearch\n    url: https://localhost \n    index: logs \n    httpOptions:\n      key: /ssl-keys/client.key\n      cert: /ssl-keys/client.crt\n      rejectUnauthorized: true", 
            "title": "HTTP and HTTPS options"
        }, 
        {
            "location": "/filters/", 
            "text": "Filters\n\n\nFilters can drop, transform or aggregate log events and hook into the processing chain. \n\n\nThere are two types of filters:\n- Input filters - process raw input from input plugins before log events get parsed\n- Output filters - process parsed log events before they are passed to output plugins.\n\n\nInput Plugins -\n \nInput Filters\n -\n Parser -\n \nOutput Filter\n -\n Output Plugins\n\n\nExample: \n\n\n1. Input: Tail Web Server Log -g '/var/log/httpd/access.log'\n2. Input-Filter: Grep URL's of interest 'login|register|upgrade'   \n3. Parser: Parse Log and generate fileds like URL, status code, size, referer, country etc.\n5. Output Filter: Drop non-relevant log events like redirects (status=302)\n6. Output Plugin: Store filtered log events in Elasticsearch\n\n\n\n\nFilters can be declared inline as JavaScript in function or as reference to a npm modules in Logagent config file. \n\n\nInput filter\n\n\nFunction parameters for input filters:\n\n\n\n\nsourceName - the name of the log source e.g. '/var/log/httpd/access.log'\n\n\nconfig - the configuration options from the config file \n\n\ndata - the raw (input filter) or parsed data (output filter)\n\n\ncallback - MUST be called. \n\n\ncallback() without parameters drops the event. \n\n\ncallback (null,data) will pass the log event to the next filter or output plugin. \n\n\ncallback(error) will report an error and drops the event\n\n\n\n\nNode.js modules can be loaded as filter function with the \nmodule\n keyword.\nA module can be declared inline as JavaScript function using \n!!js/function \n in the module property. Properties in the config section are passed to the filter function as \"config\" object.\n\n\nExample, using npm modules: \n\n\ninputFilter:\n  - module: logagent-filter-input-grep\n    config:\n      matchSource: !!js/regexp /myapp.log/\n      include: !!js/regexp /info|error/i\n      exclude: !!js/regexp /test/i\n\n\n\n\nExample, inline JavaScript function:\n\n\ninputFilter:\n  - module: logagent-filter-input-grep\n    config:\n      matchSource: !!js/regexp /myapp.log/\n      include: !!js/regexp /info|error/i\n      exclude: !!js/regexp /test/i\n    module: !!js/function \n \n        function (sourceName, config, data, callback) {\n              try {\n                var drop = false\n                if (config.matchSource) {\n                  if (!config.matchSource.test(sourceName)) {\n                    // pass data for unmatched source names\n                    return callback(null, data)\n                  }\n                }\n                // filter data for matched source names\n                if (config.include) {\n                  drop = !config.include.test(data)\n                }\n                if (config.exclude) {\n                  drop = config.exclude.test(data) || drop\n                }\n                drop ? callback() : callback(null, data)\n              } catch (err) {\n                return callback(null, data)\n              }\n            }\n\n\n\n\nOutput filter\n\n\nFunction parameters for output filters:\n\n\n\n\ncontext - an object providing information about the log source, e.g. context.source \n\n\nconfig - the configuration options from the config file\n\n\neventEmitter - the eventEmitter send new events to logagent plugins emit('data.parsed', context, data). Required for aggregation plugins, which typicall drop all events and generate new events with aggregated stats. \n\n\ndata - the raw (input filter) or parsed data (output filter)\n\n\ncallback - MUST be called. \n\n\ncallback() without parameters drops the event. \n\n\ncallback (null,data) will pass the log event to the next filter or output plugin. \n\n\ncallback(error) will report an error and drops the event\n\n\n\n\nNode.js modules can be loaded as filter function with the \nmodule\n keyword.\nA module can be declared inline as JavaScript function using \n!!js/function \n in the module property. Properties in the config section are passed to the filter function as \"config\" object.\n\n\nExample, inline declaration to implement the grep filter from above applied to data.message field. \n\n\noutputFilter:\n  - config:\n      matchSource: !!js/regexp /myapp.log/\n      include: !!js/regexp /info|error/i\n      exclude: !!js/regexp /test/i\n    module: !!js/function \n \n        function (context, config, eventEmitter, data, callback)  {\n              try {\n                var sourceName = context.source\n                var drop = false\n                if (config.matchSource) {\n                  if (!config.matchSource.test(sourceName)) {\n                    // pass data for unmatched source names\n                    return callback(null, data)\n                  }\n                }\n                // filter data for matched source names\n                if (config.include) {\n                  drop = !config.include.test(data.message)\n                }\n                if (config.exclude) {\n                  drop = config.exclude.test(data) || drop\n                }\n                drop ? callback() : callback(null, data)\n              } catch (err) {\n                // pass all events to next filter\n                return callback(null, data)\n              }\n            }\n\n\n\n\nList of available filters\n\n\n\n\nGrep input filter\n - module alias \"grep\"\n\n\nSQL output filter\n - module alias \"sql\"", 
            "title": "About filters"
        }, 
        {
            "location": "/filters/#filters", 
            "text": "Filters can drop, transform or aggregate log events and hook into the processing chain.   There are two types of filters:\n- Input filters - process raw input from input plugins before log events get parsed\n- Output filters - process parsed log events before they are passed to output plugins.  Input Plugins -   Input Filters  -  Parser -   Output Filter  -  Output Plugins  Example:   1. Input: Tail Web Server Log -g '/var/log/httpd/access.log'\n2. Input-Filter: Grep URL's of interest 'login|register|upgrade'   \n3. Parser: Parse Log and generate fileds like URL, status code, size, referer, country etc.\n5. Output Filter: Drop non-relevant log events like redirects (status=302)\n6. Output Plugin: Store filtered log events in Elasticsearch  Filters can be declared inline as JavaScript in function or as reference to a npm modules in Logagent config file.", 
            "title": "Filters"
        }, 
        {
            "location": "/filters/#input-filter", 
            "text": "Function parameters for input filters:   sourceName - the name of the log source e.g. '/var/log/httpd/access.log'  config - the configuration options from the config file   data - the raw (input filter) or parsed data (output filter)  callback - MUST be called.   callback() without parameters drops the event.   callback (null,data) will pass the log event to the next filter or output plugin.   callback(error) will report an error and drops the event   Node.js modules can be loaded as filter function with the  module  keyword.\nA module can be declared inline as JavaScript function using  !!js/function   in the module property. Properties in the config section are passed to the filter function as \"config\" object.  Example, using npm modules:   inputFilter:\n  - module: logagent-filter-input-grep\n    config:\n      matchSource: !!js/regexp /myapp.log/\n      include: !!js/regexp /info|error/i\n      exclude: !!js/regexp /test/i  Example, inline JavaScript function:  inputFilter:\n  - module: logagent-filter-input-grep\n    config:\n      matchSource: !!js/regexp /myapp.log/\n      include: !!js/regexp /info|error/i\n      exclude: !!js/regexp /test/i\n    module: !!js/function   \n        function (sourceName, config, data, callback) {\n              try {\n                var drop = false\n                if (config.matchSource) {\n                  if (!config.matchSource.test(sourceName)) {\n                    // pass data for unmatched source names\n                    return callback(null, data)\n                  }\n                }\n                // filter data for matched source names\n                if (config.include) {\n                  drop = !config.include.test(data)\n                }\n                if (config.exclude) {\n                  drop = config.exclude.test(data) || drop\n                }\n                drop ? callback() : callback(null, data)\n              } catch (err) {\n                return callback(null, data)\n              }\n            }", 
            "title": "Input filter"
        }, 
        {
            "location": "/filters/#output-filter", 
            "text": "Function parameters for output filters:   context - an object providing information about the log source, e.g. context.source   config - the configuration options from the config file  eventEmitter - the eventEmitter send new events to logagent plugins emit('data.parsed', context, data). Required for aggregation plugins, which typicall drop all events and generate new events with aggregated stats.   data - the raw (input filter) or parsed data (output filter)  callback - MUST be called.   callback() without parameters drops the event.   callback (null,data) will pass the log event to the next filter or output plugin.   callback(error) will report an error and drops the event   Node.js modules can be loaded as filter function with the  module  keyword.\nA module can be declared inline as JavaScript function using  !!js/function   in the module property. Properties in the config section are passed to the filter function as \"config\" object.  Example, inline declaration to implement the grep filter from above applied to data.message field.   outputFilter:\n  - config:\n      matchSource: !!js/regexp /myapp.log/\n      include: !!js/regexp /info|error/i\n      exclude: !!js/regexp /test/i\n    module: !!js/function   \n        function (context, config, eventEmitter, data, callback)  {\n              try {\n                var sourceName = context.source\n                var drop = false\n                if (config.matchSource) {\n                  if (!config.matchSource.test(sourceName)) {\n                    // pass data for unmatched source names\n                    return callback(null, data)\n                  }\n                }\n                // filter data for matched source names\n                if (config.include) {\n                  drop = !config.include.test(data.message)\n                }\n                if (config.exclude) {\n                  drop = config.exclude.test(data) || drop\n                }\n                drop ? callback() : callback(null, data)\n              } catch (err) {\n                // pass all events to next filter\n                return callback(null, data)\n              }\n            }", 
            "title": "Output filter"
        }, 
        {
            "location": "/filters/#list-of-available-filters", 
            "text": "Grep input filter  - module alias \"grep\"  SQL output filter  - module alias \"sql\"", 
            "title": "List of available filters"
        }, 
        {
            "location": "/input-filter-grep/", 
            "text": "Grep Input Filter\n\n\nApply regex to filter raw input from @sematext/logagent before logs are parsed\n\n\nConfiguration\n\n\nAdd following section to @sematext/logagent configuration file. Please note you could use the plugin with multiple configurations. Output of the first filter is passed into the next one ...: \n\n\ninput: \n  files:\n    - '/var/log/**/*.log'\n\ninputFilter:\n  - module: grep\n    config:\n      matchSource: !!js/regexp /myapp.log/\n      include: !!js/regexp /info|error/i\n      exclude: !!js/regexp /test/i\n\noutput:\n  elasticsearch:\n    url: http://localhost:9200\n    index: mylogs\n\n\n\n\n\nThe example above filters all log files with the content \"info\" or \"error\", and drops all lines with the keyword \"test\". \n\n\nRun logagent: \n\n\nlogagent --config myconfig.yml", 
            "title": "Grep input filter"
        }, 
        {
            "location": "/input-filter-grep/#grep-input-filter", 
            "text": "Apply regex to filter raw input from @sematext/logagent before logs are parsed", 
            "title": "Grep Input Filter"
        }, 
        {
            "location": "/input-filter-grep/#configuration", 
            "text": "Add following section to @sematext/logagent configuration file. Please note you could use the plugin with multiple configurations. Output of the first filter is passed into the next one ...:   input: \n  files:\n    - '/var/log/**/*.log'\n\ninputFilter:\n  - module: grep\n    config:\n      matchSource: !!js/regexp /myapp.log/\n      include: !!js/regexp /info|error/i\n      exclude: !!js/regexp /test/i\n\noutput:\n  elasticsearch:\n    url: http://localhost:9200\n    index: mylogs  The example above filters all log files with the content \"info\" or \"error\", and drops all lines with the keyword \"test\".   Run logagent:   logagent --config myconfig.yml", 
            "title": "Configuration"
        }, 
        {
            "location": "/output-filter-sql/", 
            "text": "SQL output filter\n\n\nFilter and aggregate parsed logs with SQL \n\n\nThis applies SQL queries on parsed log events. The result of the query is emitted as new event, while the original events are omitted. \n\n\nUsing SQL it is very easy to aggregate values, e.g. group HTTP requests by status codes. The SQL WHERE statement is useful to filter events, before they get shipped to Elasticsearch or \nLogsene\n. \n\n\nConfiguration\n\n\nAdd following section 'outputFilter' to @sematext/logagent configuration file. Please note you could use the plugin with multiple configurations for different event sources. \n\n\ninput: \n  files:\n    - '/var/log/*/access.log'\n\noutputFilter:\n  - module: sql\n    config:\n      source: !!js/regexp /access.log|httpd/\n      interval: 1 # every second\n      queries:\n        - # calculate average page size for different HTTP methods\n          SELECT 'apache_stats' AS _type, \n                  AVG(size) AS size_avg, \n                  COUNT(method) AS method_count, \n                  method as http_method\n          FROM ? \n          GROUP BY method\n        - # log each request to the login page \n          SELECT * \n          FROM ? \n          WHERE path like \n/wp-login%\n \noutput:\n  elasticsearch:\n    url: http://localhost:9200\n    index: mylogs\n\n\n\n\nRun logagent with your config: \n\n\nlogagent --config logagent-example-config.yml", 
            "title": "SQL output filter"
        }, 
        {
            "location": "/output-filter-sql/#sql-output-filter", 
            "text": "Filter and aggregate parsed logs with SQL   This applies SQL queries on parsed log events. The result of the query is emitted as new event, while the original events are omitted.   Using SQL it is very easy to aggregate values, e.g. group HTTP requests by status codes. The SQL WHERE statement is useful to filter events, before they get shipped to Elasticsearch or  Logsene .", 
            "title": "SQL output filter"
        }, 
        {
            "location": "/output-filter-sql/#configuration", 
            "text": "Add following section 'outputFilter' to @sematext/logagent configuration file. Please note you could use the plugin with multiple configurations for different event sources.   input: \n  files:\n    - '/var/log/*/access.log'\n\noutputFilter:\n  - module: sql\n    config:\n      source: !!js/regexp /access.log|httpd/\n      interval: 1 # every second\n      queries:\n        - # calculate average page size for different HTTP methods\n          SELECT 'apache_stats' AS _type, \n                  AVG(size) AS size_avg, \n                  COUNT(method) AS method_count, \n                  method as http_method\n          FROM ? \n          GROUP BY method\n        - # log each request to the login page \n          SELECT * \n          FROM ? \n          WHERE path like  /wp-login%  \noutput:\n  elasticsearch:\n    url: http://localhost:9200\n    index: mylogs  Run logagent with your config:   logagent --config logagent-example-config.yml", 
            "title": "Configuration"
        }
    ]
}